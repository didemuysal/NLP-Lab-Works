{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-sigma",
   "metadata": {
    "id": "happy-sigma"
   },
   "source": [
    "# Lab 01 - Tokenisation and basic feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-supervision",
   "metadata": {
    "id": "downtown-supervision"
   },
   "source": [
    "This lab is an introduction to some basic test processing that will give us the first impression of the challenges in translating text into meaningful feature vector representations.\n",
    "\n",
    "So let's first set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-throat",
   "metadata": {
    "id": "stopped-throat"
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\" Welcome to the Natural Language Processing lab!\n",
    "               We'll learn many things in this no 1 lab, so we will take it easy.\n",
    "               Natural Language Processing is fun.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86574662",
   "metadata": {
    "id": "86574662"
   },
   "source": [
    "Let's also download some libraries we'll be using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKciRQkx0Vze",
   "metadata": {
    "id": "aKciRQkx0Vze"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76028587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76028587",
    "outputId": "8b33a989-1d49-4941-c6d7-b5203807ffc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\uysal\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\uysal\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\uysal\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\uysal\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f80b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28da2c66",
   "metadata": {
    "id": "28da2c66"
   },
   "source": [
    "## Using native Python functions for tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-perry",
   "metadata": {
    "id": "southwest-perry"
   },
   "source": [
    "Let's try to split the text based on spaces using the built in string function `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "careful-cleaning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "careful-cleaning",
    "outputId": "d9c3129f-2814-4205-bc76-ffc6424ede22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab!',\n",
       " \"We'll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab,',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-brain",
   "metadata": {
    "id": "mounted-brain"
   },
   "source": [
    "You will observe that some words are not well separated from punctuation and contain some appended onto them.\n",
    "So we need to find a way to remove those characters... but, before we do that, let's see how we can create a quick feature vector first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regional-services",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "regional-services",
    "outputId": "96506f22-3267-49ea-848f-5f3ac2ffe05a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Language',\n",
       " 'Natural',\n",
       " 'Processing',\n",
       " \"We'll\",\n",
       " 'Welcome',\n",
       " 'easy.',\n",
       " 'fun.',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'lab!',\n",
       " 'lab,',\n",
       " 'learn',\n",
       " 'many',\n",
       " 'no',\n",
       " 'so',\n",
       " 'take',\n",
       " 'the',\n",
       " 'things',\n",
       " 'this',\n",
       " 'to',\n",
       " 'we',\n",
       " 'will']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sentence.split()  # splitting based on spaces\n",
    "vocab = sorted(set(tokens))  # sorting and removing duplicates by using set()\n",
    "vocab  # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syndication",
   "metadata": {
    "id": "hundred-syndication"
   },
   "source": [
    "We can see that the sorted list has the numbers first, followed by capital and then lower case letters (all alphabetically sorted). We also see that repeating words appear only once in our vocabulary list. Let's compare the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "periodic-typing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "periodic-typing",
    "outputId": "0a4b53a0-9ef0-4b86-fe2d-0a22b19dbe95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 27\n",
      "Vocab: 24\n"
     ]
    }
   ],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(f\"Tokens: {tokens_len}\")\n",
    "print(f\"Vocab: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-private",
   "metadata": {
    "id": "published-private"
   },
   "source": [
    "Let's try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ultimate-litigation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ultimate-litigation",
    "outputId": "beaf4516-8cd4-4277-8d21-eff58da3a684"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-beach",
   "metadata": {
    "id": "designed-beach"
   },
   "source": [
    "It's not easy to see, but the second, third and fourth columns have the value of 1 in two rows, whereas the rest only take the value of 1 in a single row. To make it a little more readable, we could use Pandas and DataFrame! Both Pandas and NumPy are very useful libs that we will use many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "realistic-wallace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "realistic-wallace",
    "outputId": "31ba3188-1fe2-4349-e150-1474c98e4a07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1e91afb1-e5fb-4e20-9ea0-1c4c647a5bf3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>Language</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We'll</th>\n",
       "      <th>Welcome</th>\n",
       "      <th>easy.</th>\n",
       "      <th>fun.</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>many</th>\n",
       "      <th>no</th>\n",
       "      <th>so</th>\n",
       "      <th>take</th>\n",
       "      <th>the</th>\n",
       "      <th>things</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows Ã— 24 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e91afb1-e5fb-4e20-9ea0-1c4c647a5bf3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1e91afb1-e5fb-4e20-9ea0-1c4c647a5bf3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1e91afb1-e5fb-4e20-9ea0-1c4c647a5bf3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-5e7931d6-1fed-491d-bb2a-ffa0391c4b93\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e7931d6-1fed-491d-bb2a-ffa0391c4b93')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-5e7931d6-1fed-491d-bb2a-ffa0391c4b93 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    1  Language  Natural  Processing  We'll  Welcome  easy.  fun.  in  is  \\\n",
       "0   0         0        0           0      0        1      0     0   0   0   \n",
       "1   0         0        0           0      0        0      0     0   0   0   \n",
       "2   0         0        0           0      0        0      0     0   0   0   \n",
       "3   0         0        1           0      0        0      0     0   0   0   \n",
       "4   0         1        0           0      0        0      0     0   0   0   \n",
       "5   0         0        0           1      0        0      0     0   0   0   \n",
       "6   0         0        0           0      0        0      0     0   0   0   \n",
       "7   0         0        0           0      1        0      0     0   0   0   \n",
       "8   0         0        0           0      0        0      0     0   0   0   \n",
       "9   0         0        0           0      0        0      0     0   0   0   \n",
       "10  0         0        0           0      0        0      0     0   0   0   \n",
       "11  0         0        0           0      0        0      0     0   1   0   \n",
       "12  0         0        0           0      0        0      0     0   0   0   \n",
       "13  0         0        0           0      0        0      0     0   0   0   \n",
       "14  1         0        0           0      0        0      0     0   0   0   \n",
       "15  0         0        0           0      0        0      0     0   0   0   \n",
       "16  0         0        0           0      0        0      0     0   0   0   \n",
       "17  0         0        0           0      0        0      0     0   0   0   \n",
       "18  0         0        0           0      0        0      0     0   0   0   \n",
       "19  0         0        0           0      0        0      0     0   0   0   \n",
       "20  0         0        0           0      0        0      0     0   0   0   \n",
       "21  0         0        0           0      0        0      1     0   0   0   \n",
       "22  0         0        1           0      0        0      0     0   0   0   \n",
       "23  0         1        0           0      0        0      0     0   0   0   \n",
       "24  0         0        0           1      0        0      0     0   0   0   \n",
       "25  0         0        0           0      0        0      0     0   0   1   \n",
       "26  0         0        0           0      0        0      0     1   0   0   \n",
       "\n",
       "    ...  many  no  so  take  the  things  this  to  we  will  \n",
       "0   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "1   ...     0   0   0     0    0       0     0   1   0     0  \n",
       "2   ...     0   0   0     0    1       0     0   0   0     0  \n",
       "3   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "4   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "5   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "6   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "7   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "8   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "9   ...     1   0   0     0    0       0     0   0   0     0  \n",
       "10  ...     0   0   0     0    0       1     0   0   0     0  \n",
       "11  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "12  ...     0   0   0     0    0       0     1   0   0     0  \n",
       "13  ...     0   1   0     0    0       0     0   0   0     0  \n",
       "14  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "15  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "16  ...     0   0   1     0    0       0     0   0   0     0  \n",
       "17  ...     0   0   0     0    0       0     0   0   1     0  \n",
       "18  ...     0   0   0     0    0       0     0   0   0     1  \n",
       "19  ...     0   0   0     1    0       0     0   0   0     0  \n",
       "20  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "21  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "22  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "23  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "24  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "25  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "26  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "\n",
       "[27 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-barrel",
   "metadata": {
    "id": "increasing-barrel"
   },
   "source": [
    "Now this is a lot more clear and if we wanted we could carry on making it look nicer.\n",
    "\n",
    "Let's now carry on building the bag of words (BoW) -\n",
    "- *Bags of words (BoW) are a basic representation technique used in NLP.*\n",
    "- *Its main purpose is to convert textual data into a numerical format that can be used for further analysis.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "anonymous-anaheim",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anonymous-anaheim",
    "outputId": "f2ab60c0-016a-4d9f-b23b-e6ae2929ab21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('Language', 1),\n",
       " ('Natural', 1),\n",
       " ('Processing', 1),\n",
       " (\"We'll\", 1),\n",
       " ('Welcome', 1),\n",
       " ('easy.', 1),\n",
       " ('fun.', 1),\n",
       " ('in', 1),\n",
       " ('is', 1),\n",
       " ('it', 1),\n",
       " ('lab!', 1),\n",
       " ('lab,', 1),\n",
       " ('learn', 1),\n",
       " ('many', 1),\n",
       " ('no', 1),\n",
       " ('so', 1),\n",
       " ('take', 1),\n",
       " ('the', 1),\n",
       " ('things', 1),\n",
       " ('this', 1),\n",
       " ('to', 1),\n",
       " ('we', 1),\n",
       " ('will', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = {token: 1 for token in tokens}  # setting this up as a dictionary\n",
    "sorted(bow.items())  # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palestine",
   "metadata": {
    "id": "better-palestine"
   },
   "source": [
    "Since bow is a dictionary, we see that there are no duplicate words.\n",
    "\n",
    "Pandas also has a more efficient form of a dictionary called `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adequate-aquatic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "adequate-aquatic",
    "outputId": "e1b69c4b-a32b-4567-fa69-7f26dc14d934"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-76560c2f-a817-4093-bf43-17e8a8268c72\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Welcome</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Language</th>\n",
       "      <th>Processing</th>\n",
       "      <th>lab!</th>\n",
       "      <th>We'll</th>\n",
       "      <th>learn</th>\n",
       "      <th>many</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>lab,</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>take</th>\n",
       "      <th>it</th>\n",
       "      <th>easy.</th>\n",
       "      <th>is</th>\n",
       "      <th>fun.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76560c2f-a817-4093-bf43-17e8a8268c72')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-76560c2f-a817-4093-bf43-17e8a8268c72 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-76560c2f-a817-4093-bf43-17e8a8268c72');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      Welcome  to  the  Natural  Language  Processing  lab!  We'll  learn  \\\n",
       "sent        1   1    1        1         1           1     1      1      1   \n",
       "\n",
       "      many  ...  1  lab,  so  we  will  take  it  easy.  is  fun.  \n",
       "sent     1  ...  1     1   1   1     1     1   1      1   1     1  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=[\"sent\"]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "senior-shareware",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "senior-shareware",
    "outputId": "3f40f941-843d-480a-90a1-331fe9c4dc66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-06c4b0dd-b075-4602-9934-191020afe101\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Welcome</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Language</th>\n",
       "      <th>Processing</th>\n",
       "      <th>lab!</th>\n",
       "      <th>We'll</th>\n",
       "      <th>learn</th>\n",
       "      <th>many</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>lab,</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>take</th>\n",
       "      <th>it</th>\n",
       "      <th>easy.</th>\n",
       "      <th>is</th>\n",
       "      <th>fun.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06c4b0dd-b075-4602-9934-191020afe101')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-06c4b0dd-b075-4602-9934-191020afe101 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-06c4b0dd-b075-4602-9934-191020afe101');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c6410f65-9d6f-4a77-891a-f714d496654d\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6410f65-9d6f-4a77-891a-f714d496654d')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c6410f65-9d6f-4a77-891a-f714d496654d button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       Welcome  to  the  Natural  Language  Processing  lab!  We'll  learn  \\\n",
       "sent0        1   1    1        1         1           1     1      0      0   \n",
       "sent1        0   0    0        0         0           0     0      1      1   \n",
       "sent2        0   0    0        1         1           1     0      0      0   \n",
       "\n",
       "       many  ...  1  lab,  so  we  will  take  it  easy.  is  fun.  \n",
       "sent0     0  ...  0     0   0   0     0     0   0      0   0     0  \n",
       "sent1     1  ...  1     1   1   1     1     1   1      1   0     0  \n",
       "sent2     0  ...  0     0   0   0     0     0   0      0   1     1  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus[f\"sent{i}\"] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rachel",
   "metadata": {
    "id": "every-rachel"
   },
   "source": [
    "Now we see how we managed to build feature vectors for the three sentences we originally had. Now let's do a Dot Product calculation.\n",
    "\n",
    "- *In the context of measuring similarity, the dot product is often used to quantify the **similarity** between two vectors.*\n",
    "- *The **higher** the dot product, the more **similar** the vectors are considered to be.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "computational-tyler",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "computational-tyler",
    "outputId": "c9121871-ec41-4439-ace7-55fe481afedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of sent0 from sent1: 0 and dot product of sent0 from sent2: 3\n"
     ]
    }
   ],
   "source": [
    "df = df.T\n",
    "print(f\"Dot product of sent0 from sent1: {df.sent0.dot(df.sent1)} and dot product of sent0 from sent2: {df.sent0.dot(df.sent2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-pavilion",
   "metadata": {
    "id": "vocational-pavilion"
   },
   "source": [
    "As we see from the results, the higher the dot product to more similar the vectors are... so given that only the first and last sentence have some common words, we see that this comes back as 3, where as the two sentences who have nothing in common come bak as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-realtor",
   "metadata": {
    "id": "arabic-realtor"
   },
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Let's do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "saving-fifty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saving-fifty",
    "outputId": "0dd7dd9e-2f4e-49c3-d0d3-7d6a781949d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " \"We'll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r\"[-\\s.,;!?]+\", sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710aa204",
   "metadata": {
    "id": "710aa204"
   },
   "source": [
    "## NLTK\n",
    "\n",
    "Although this seems to be great... you might still have issues with different characters that are not anticipated. So we usually use an existing NLP related tokeniser to do this job. Let's try the NLTK lib.\n",
    "\n",
    "NLTK also supports regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prime-limit",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prime-limit",
    "outputId": "2562cc7f-6080-4546-a9fb-52af9482421d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|$[0-9.]+|\\S+\")\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prototype",
   "metadata": {
    "id": "handmade-prototype"
   },
   "source": [
    "but there are other more specialised tokenisers, such as the *Treebank Word Tokenizer*:\n",
    "\n",
    "\n",
    "- *It is specifically designed to tokenize text in a manner similar to the **Penn Treebank Tokenization conventions**.*\n",
    "- *The Penn Treebank is a **large corpus** of English text, and the tokenization conventions used in its processing have become a standard for various NLP tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rapid-dictionary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rapid-dictionary",
    "outputId": "bdcbcac3-9fbe-4567-9682-65233e032054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-graphic",
   "metadata": {
    "id": "adjusted-graphic"
   },
   "source": [
    "For now let's use the regular expression special word pattern `\\w`, so we can control what the tokeniser does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "south-engine",
   "metadata": {
    "id": "south-engine",
    "outputId": "12ff114e-2f8a-4d06-d51e-dcbabd74b709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'to', 'the', 'Natural', 'Language', 'Processing', 'lab', 'We', 'll', 'learn', 'many', 'things', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'Natural', 'Language', 'Processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#\\w is a special sequence in regular expressions that matches any alphanumeric character, including letters (both uppercase and lowercase) and digits.\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-category",
   "metadata": {
    "id": "loved-category"
   },
   "source": [
    "At the point you could try out different other tokenisers from other libraries and see if there are any differences.\n",
    "\n",
    "We will now calculate the 2-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "guilty-export",
   "metadata": {
    "id": "guilty-export",
    "outputId": "3a0e4a78-aade-4ddc-9762-70399f02e81e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'Natural'),\n",
       " ('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'lab'),\n",
       " ('lab', 'We'),\n",
       " ('We', 'll'),\n",
       " ('ll', 'learn'),\n",
       " ('learn', 'many'),\n",
       " ('many', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'this'),\n",
       " ('this', 'no'),\n",
       " ('no', '1'),\n",
       " ('1', 'lab'),\n",
       " ('lab', 'so'),\n",
       " ('so', 'we'),\n",
       " ('we', 'will'),\n",
       " ('will', 'take'),\n",
       " ('take', 'it'),\n",
       " ('it', 'easy'),\n",
       " ('easy', 'Natural'),\n",
       " ('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'is'),\n",
       " ('is', 'fun')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-present",
   "metadata": {
    "id": "connected-present"
   },
   "source": [
    "and 3-grams:\n",
    "\n",
    "These *$n$-grams* are a contiguous sequence of $n$ items from a given sample of text or speech. They can be used to **identify** common phrases or expressions and can be valuable in tasks like language modeling, where predicting the next word is based on the preceding one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imperial-retreat",
   "metadata": {
    "id": "imperial-retreat",
    "outputId": "ccc644f5-7f28-4c25-dd42-bd085b71e297"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'to', 'the'),\n",
       " ('to', 'the', 'Natural'),\n",
       " ('the', 'Natural', 'Language'),\n",
       " ('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'lab'),\n",
       " ('Processing', 'lab', 'We'),\n",
       " ('lab', 'We', 'll'),\n",
       " ('We', 'll', 'learn'),\n",
       " ('ll', 'learn', 'many'),\n",
       " ('learn', 'many', 'things'),\n",
       " ('many', 'things', 'in'),\n",
       " ('things', 'in', 'this'),\n",
       " ('in', 'this', 'no'),\n",
       " ('this', 'no', '1'),\n",
       " ('no', '1', 'lab'),\n",
       " ('1', 'lab', 'so'),\n",
       " ('lab', 'so', 'we'),\n",
       " ('so', 'we', 'will'),\n",
       " ('we', 'will', 'take'),\n",
       " ('will', 'take', 'it'),\n",
       " ('take', 'it', 'easy'),\n",
       " ('it', 'easy', 'Natural'),\n",
       " ('easy', 'Natural', 'Language'),\n",
       " ('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'is'),\n",
       " ('Processing', 'is', 'fun')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-shooting",
   "metadata": {
    "id": "aggregate-shooting"
   },
   "source": [
    "If we want to include the n-grams as strings rather than tuples, then we need to convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "twenty-enhancement",
   "metadata": {
    "id": "twenty-enhancement",
    "outputId": "94a2584b-2875-4a10-e45c-ab5b2030571b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['Welcome to', 'to the', 'the Natural', 'Natural Language', 'Language Processing', 'Processing lab', 'lab We', 'We ll', 'll learn', 'learn many', 'many things', 'things in', 'in this', 'this no', 'no 1', '1 lab', 'lab so', 'so we', 'we will', 'will take', 'take it', 'it easy', 'easy Natural', 'Natural Language', 'Language Processing', 'Processing is', 'is fun']\n",
      "\n",
      "Trigrams: ['Welcome to the', 'to the Natural', 'the Natural Language', 'Natural Language Processing', 'Language Processing lab', 'Processing lab We', 'lab We ll', 'We ll learn', 'll learn many', 'learn many things', 'many things in', 'things in this', 'in this no', 'this no 1', 'no 1 lab', '1 lab so', 'lab so we', 'so we will', 'we will take', 'will take it', 'take it easy', 'it easy Natural', 'easy Natural Language', 'Natural Language Processing', 'Language Processing is', 'Processing is fun']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(f\"Bigrams: {bigrams}\\n\")\n",
    "\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print(f\"Trigrams: {trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-syracuse",
   "metadata": {
    "id": "seven-syracuse"
   },
   "source": [
    "Another important step we looked at in the lectures are the stop words. Let's try to use the nltk stop word list to remove them.\n",
    "\n",
    "These are words that are *commonly used* in a language but are typically *filtered out* during text processing because they are considered to carry little to no meaning on their own.\n",
    "\n",
    "First, let's download the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apparent-going",
   "metadata": {
    "id": "apparent-going",
    "outputId": "1df273c0-ee79-4814-f2d9-dde77b8f3c3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\uysal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-disorder",
   "metadata": {
    "id": "annual-disorder"
   },
   "source": [
    "and now check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pending-reducing",
   "metadata": {
    "id": "pending-reducing",
    "outputId": "8df3e6c6-fa3f-40ef-dc16-6e1d7335fe52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(f\"number of stopwords: {len(stop_words)}\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-expression",
   "metadata": {
    "id": "friendly-expression"
   },
   "source": [
    "Other libs have different stopwords. Let's see a much larger set from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "monthly-armenia",
   "metadata": {
    "id": "monthly-armenia",
    "outputId": "199bca11-3c7d-4b97-fb9b-0c1d8bdc2c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 318\n",
      "frozenset({'somewhere', 'amount', 'become', 'often', 'although', 'yours', 'such', 'from', 'while', 'top', 'beyond', 'never', 'meanwhile', 'ever', 'whole', 'couldnt', 'itself', 'why', 'had', 'keep', 'against', 'perhaps', 'fifty', 'she', 'would', 'bottom', 'whither', 'always', 'no', 'seemed', 'sometime', 'sometimes', 'to', 'within', 'an', 'however', 'nevertheless', 'thereafter', 'off', 'mine', 'before', 'without', 'in', 'your', 'inc', 'etc', 'moreover', 'anyhow', 'cry', 'was', 'been', 'thence', 'us', 'what', 'whenever', 'formerly', 'at', 'two', 'whatever', 'everyone', 'hereupon', 'even', 'give', 'eight', 'only', 'one', 'those', 'wherein', 'somehow', 'own', 'co', 'up', 'being', 'might', 'has', 'above', 'who', 'neither', 'seeming', 'hereby', 'third', 'less', 'forty', 'several', 'thereupon', 'sincere', 'where', 'whence', 'ourselves', 'can', 'these', 'the', 'you', 'now', 'besides', 'latterly', 'all', 'our', 'ltd', 'could', 'front', 'he', 'ours', 'find', 'eleven', 'here', 'mostly', 'under', 'per', 'another', 'whoever', 'its', 'part', 'least', 'there', 'they', 'show', 'must', 'therefore', 'him', 'nor', 'due', 'becoming', 'with', 'will', 'again', 'her', 'system', 'amoungst', 'my', 'not', 'by', 'may', 'is', 're', 'their', 'already', 'whose', 'were', 'some', 'about', 'for', 'describe', 'on', 'fire', 'go', 'found', 'rather', 'do', 'becomes', 'further', 'or', 'most', 'well', 'move', 'once', 'empty', 'full', 'everything', 'six', 'name', 'himself', 'done', 'twenty', 'anything', 'so', 'thick', 'elsewhere', 'others', 'thin', 'are', 'anywhere', 'whom', 'thru', 'hence', 'noone', 'any', 'twelve', 'alone', 'interest', 'that', 'i', 'afterwards', 'something', 'please', 'thereby', 'when', 'than', 'see', 'other', 'side', 'onto', 'as', 'whereby', 'thus', 'of', 'nowhere', 'across', 'into', 'how', 'last', 'wherever', 'eg', 'very', 'next', 'between', 'after', 'yet', 'beside', 'this', 'call', 'and', 'towards', 'therein', 'hereafter', 'we', 'cannot', 'either', 'beforehand', 'since', 'hasnt', 'fill', 'toward', 'ie', 'fifteen', 'seems', 'down', 'during', 'everywhere', 'whereupon', 'over', 'herein', 'both', 'un', 'together', 'serious', 'amongst', 'same', 'namely', 'many', 'his', 'yourselves', 'yourself', 'which', 'themselves', 'herself', 'none', 'seem', 'nine', 'first', 'except', 'three', 'a', 'because', 'more', 'four', 'via', 'put', 'them', 'it', 'almost', 'take', 'five', 'myself', 'then', 'too', 'con', 'below', 'get', 'nobody', 'among', 'someone', 'much', 'indeed', 'latter', 'sixty', 'nothing', 'whereas', 'hundred', 'ten', 'until', 'made', 'me', 'each', 'former', 'have', 'mill', 'should', 'de', 'anyway', 'every', 'anyone', 'became', 'though', 'through', 'but', 'few', 'back', 'otherwise', 'hers', 'still', 'along', 'around', 'else', 'if', 'whether', 'am', 'bill', 'be', 'also', 'throughout', 'cant', 'out', 'behind', 'detail', 'whereafter', 'enough', 'upon'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(f\"number of stopwords: {len(sklearn_stop_words)}\")\n",
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-deviation",
   "metadata": {
    "id": "exempt-deviation"
   },
   "source": [
    "Strangely enough, although there are more stop words in sklearn, you will find that nltk has words that are not contained in sklearn. So you might want to join the two lists.\n",
    "\n",
    "For normalising the text you could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alive-employee",
   "metadata": {
    "id": "alive-employee",
    "outputId": "9d01852c-4672-43b3-8b06-85e0dbfc019d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'things', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-inflation",
   "metadata": {
    "id": "ongoing-inflation"
   },
   "source": [
    "For stemming the words we could use nltk again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "upset-reputation",
   "metadata": {
    "id": "upset-reputation",
    "outputId": "ef21a6e6-975c-41d0-c963-40efefdccc72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'to', 'the', 'natur', 'languag', 'process', 'lab', 'we', 'll', 'learn', 'mani', 'thing', 'in', 'thi', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easi', 'natur', 'languag', 'process', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-porter",
   "metadata": {
    "id": "gothic-porter"
   },
   "source": [
    "For lemmatising, nltk also does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "desperate-diploma",
   "metadata": {
    "id": "desperate-diploma",
    "outputId": "387feb38-bdda-4316-b6ac-6818e9bf0318"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\uysal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\uysal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'thing', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-relevance",
   "metadata": {
    "id": "coordinated-relevance"
   },
   "source": [
    "With this example sentence, there are no issues with the lemmatisation... but let's look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "understanding-veteran",
   "metadata": {
    "id": "understanding-veteran",
    "outputId": "0d772e42-9bdd-4984-9586-8c605d2e8c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", \"a\"))  # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-martin",
   "metadata": {
    "id": "intimate-martin"
   },
   "source": [
    "If we don't include the Part-of-speech (POS), nltk, using wordnet, does not work well. So let's try to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "russian-merit",
   "metadata": {
    "id": "russian-merit"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map the POS tag to the first character lemmatize() accepts.\"\"\"\n",
    "\n",
    "    try:  # download nltk's POS tagger if it doesn't exist\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  # use ntlk's POS tagger on the word\n",
    "\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "imperial-comparative",
   "metadata": {
    "id": "imperial-comparative",
    "outputId": "c90379a0-69fe-4f29-96d1-c8f961eb00f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\uysal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'thing', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'be', 'fun']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-assignment",
   "metadata": {
    "id": "inside-assignment"
   },
   "source": [
    "If we look at the words now we are getting more counts for our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "polished-simon",
   "metadata": {
    "id": "polished-simon",
    "outputId": "9a2cb671-e98a-4735-e883-23c5a9575d3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'natural': 2,\n",
       "         'language': 2,\n",
       "         'processing': 2,\n",
       "         'lab': 2,\n",
       "         'we': 2,\n",
       "         'welcome': 1,\n",
       "         'to': 1,\n",
       "         'the': 1,\n",
       "         'll': 1,\n",
       "         'learn': 1,\n",
       "         'many': 1,\n",
       "         'thing': 1,\n",
       "         'in': 1,\n",
       "         'this': 1,\n",
       "         'no': 1,\n",
       "         '1': 1,\n",
       "         'so': 1,\n",
       "         'will': 1,\n",
       "         'take': 1,\n",
       "         'it': 1,\n",
       "         'easy': 1,\n",
       "         'be': 1,\n",
       "         'fun': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-healthcare",
   "metadata": {
    "id": "optional-healthcare"
   },
   "source": [
    "Now let's check the most frequent 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unable-champagne",
   "metadata": {
    "id": "unable-champagne",
    "outputId": "dd98e84c-5614-483c-df75-c43a86dc75a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 2),\n",
       " ('language', 2),\n",
       " ('processing', 2),\n",
       " ('lab', 2),\n",
       " ('we', 2),\n",
       " ('welcome', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hayes",
   "metadata": {
    "id": "cross-hayes"
   },
   "source": [
    "Now let's remove the stop words and check the count again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "graduate-showcase",
   "metadata": {
    "id": "graduate-showcase",
    "outputId": "bface5c9-228a-44b0-9ca6-bf34c23a81a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'natural': 2,\n",
       "         'language': 2,\n",
       "         'processing': 2,\n",
       "         'lab': 2,\n",
       "         'welcome': 1,\n",
       "         'learn': 1,\n",
       "         'many': 1,\n",
       "         'thing': 1,\n",
       "         '1': 1,\n",
       "         'take': 1,\n",
       "         'easy': 1,\n",
       "         'fun': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-favor",
   "metadata": {
    "id": "discrete-favor"
   },
   "source": [
    "Finally... let's make our feature vector using the frequency ratio (term count / total number of terms in the doc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "retained-medicine",
   "metadata": {
    "id": "retained-medicine",
    "outputId": "eaae2f23-99eb-491f-b593-dbc158139578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 0.125, 0.125, 0.125, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spell",
   "metadata": {
    "id": "forced-spell"
   },
   "source": [
    "We have explored many many options already and we will continue with more advanced feature vectors in the next lab, plus some visualisations in charts. So until then please try different experiments on your own:\n",
    "1. See if you can change the text and have more sentences with different topics (so you can compare the feature vectors later)\n",
    "2. Try to use different libraries for tokenising, PoS tagging, stemming and lemmatising.\n",
    "3. Try to use distance metrics to compare vectors, such as Euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc8b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/92/fb/d1f0605e1e8627226c6c96053fe1632e9a04a3fbcd8b5d715528cb95eb97/spacy-3.7.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.7.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/c1/c3/dd044e6f62a3d317c461f6f0c153c6573ed13025752d779e514000c15dd2/cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.2.2 from https://files.pythonhosted.org/packages/de/a5/c242d57dc7a8afe677aa48ce370d84be3d04523cbb819c4a36b64f35155c/thinc-8.2.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.2.3-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Obtaining dependency information for typer<0.10.0,>=0.3.0 from https://files.pythonhosted.org/packages/62/39/82c9d3e10979851847361d922a373bdfef4091020da7f893acfaf07c0225/typer-0.9.4-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/fe/c3/0d04d248624a181e57c2870127dfa8d371973561caf54333c85e8f9133a2/langcodes-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\uysal\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.4-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/12.1 MB 9.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/12.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/12.1 MB 9.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/12.1 MB 10.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.5/12.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 13.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.2/12.1 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.4/12.1 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 17.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.1 MB 18.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.1 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.4/12.1 MB 19.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 479.7/479.7 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.1/6.6 MB 35.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.6 MB 26.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 26.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.7/6.6 MB 26.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 26.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 24.8 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, typer, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an example sentence.\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
